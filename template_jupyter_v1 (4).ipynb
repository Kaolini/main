{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "document.querySelector(\"#notebook-container div\").style.display = 'none';\n",
       "var timeout = 500; // #1000000ms = 1000 seconds\n",
       "function ensureKernelIsSet(timeout) {\n",
       "    var start = Date.now();\n",
       "    return new Promise(waitFor); \n",
       "    function waitFor(resolve, reject) {\n",
       "        if (Jupyter && Jupyter.notebook && Jupyter.notebook.kernel)\n",
       "            resolve(Jupyter.notebook.kernel);\n",
       "        else if (timeout && (Date.now() - start) >= timeout)\n",
       "            reject(new Error(\"timeout\"));\n",
       "        else\n",
       "            setTimeout(waitFor.bind(this, resolve, reject), 30);\n",
       "    }\n",
       "}\n",
       "\n",
       "\n",
       "ensureKernelIsSet(timeout).then(_kernel => {\n",
       "    Jupyter.notebook.execute_cells([2,4])\n",
       "});\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "document.querySelector(\"#notebook-container div\").style.display = 'none';\n",
    "var timeout = 500; // #1000000ms = 1000 seconds\n",
    "function ensureKernelIsSet(timeout) {\n",
    "    var start = Date.now();\n",
    "    return new Promise(waitFor); \n",
    "    function waitFor(resolve, reject) {\n",
    "        if (Jupyter && Jupyter.notebook && Jupyter.notebook.kernel)\n",
    "            resolve(Jupyter.notebook.kernel);\n",
    "        else if (timeout && (Date.now() - start) >= timeout)\n",
    "            reject(new Error(\"timeout\"));\n",
    "        else\n",
    "            setTimeout(waitFor.bind(this, resolve, reject), 30);\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "ensureKernelIsSet(timeout).then(_kernel => {\n",
    "    Jupyter.notebook.execute_cells([2,4])\n",
    "});\n",
    "</script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import statsmodels\n",
    "import pyspark\n",
    "import pyhive\n",
    "import subprocess\n",
    "import os\n",
    "from dsconnector import get_connect\n",
    "from fastparquet import ParquetFile\n",
    "import snappy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выбранные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "my_datasets = get_connect(datasets)\n",
    "my_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры каталогов HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главная директория выводится кодом в начале ячейки ниже, затем необходимо в последующей ячейке написать **\"!hdfs dfs -ls *путь однойиз папок из результата выполнения ячейчки ниже*\"**, эта операция выведет содержимое этой папки, необходимо будетподставлять полный путь, пока не будут доступны файлы с расширением .snappy.parquet. Полный путь до файла с этим расширением необходимо вставить вместо\"*Здесь необходимо вставить путь до файла с расширением:.snappy.parquet*\", в закомментированном коде ниже и выполнить ячейку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отображение одного из выбранных каталогов hdfs\n",
    "exs_hdfs_dir =  subprocess.run([\"hdfs\", \"dfs\", \"-ls\", f\"{my_datasets[{ID}]['Соединение']}\"], stdout=subprocess.PIPE) \n",
    "print(exs_hdfs_dir.stdout.decode('utf-8'))\n",
    "# '''\n",
    "# Для чтения файлов типа 'parquet' используйте следующую конструкцию:\n",
    "\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# from pyspark.sql import SparkSession, SQLContext\n",
    "# conf = SparkConf().setAppName('myAppName')\n",
    "# conf = conf.setAll([('spark.kryoserializer.buffer.max.mb', '1024'),\n",
    "#                    ('spark.executor.memory', '5gb'),\n",
    "#                    (\"spark.cores.max\", \"7\"),\n",
    "#                    (\"spark.sql.shuffle.partitions\",\"5\"),\n",
    "#                    (\"spark.driver.maxResultSize\",'5gb'),\n",
    "#                    (\"spark.sql.parquet.compression.codec\", \"snappy\"),\n",
    "#                    (\"spark.rdd.compress\", \"true\")])\n",
    "# sc = SparkContext(conf=conf)\n",
    "# sq = SQLContext(sc)\n",
    "# df = sq.read.parquet(\"hdfs://*Здесь необходимо вставить путь до файла с расширением:.snappy.parquet*\")\n",
    "# df.registerTempTable(\"parquet_file\")\n",
    "# df = sq.sql(\"select * from parquet_file limit 10\").collect()\n",
    "# df = pd.DataFrame(df)\n",
    "# df\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры каталогов HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример отображения данных из hive\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext \n",
    "url = f\"hive2://{my_datasets[{ID}]['Соединение']}\"\n",
    "sparkSession = (SparkSession\n",
    "                .builder\n",
    "                .appName(\"hive_table\")\n",
    "                .config(\"hive.metastore.uris\", url, conf=SparkConf())\n",
    "                .config(\"spark.master\", \"yarn\") \n",
    "                .config(\"spark.executor.memory\", \"5g\") \n",
    "                .config(\"spark.executor.cores\", 5) \n",
    "                .config(\"spark.executor.instances\", 5) \n",
    "                .enableHiveSupport()\n",
    "                .getOrCreate())\n",
    "df_load = sparkSession.sql(f\"SELECT * FROM {my_datasets[{ID}]['Таблица']} LIMIT 10\")\n",
    "df_load =  df_load.collect()\n",
    "df =pd.DataFrame(df_load)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36-spark-conda",
   "language": "python",
   "name": "python36-spark-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
